{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell should be run in order to execute the predictions. It will ask for an input. \n",
    "# Also, a name for the output file with the predictions should be given. It will be stored in the same folder as the\n",
    "# tool itself and the datasets. The accuracy and error measurement of both prediction models for event and time \n",
    "# can be found in the poster.\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import mode\n",
    "import operator\n",
    "import functools\n",
    "import datetime \n",
    "import numpy as np\n",
    "import collections, itertools\n",
    "\n",
    "def process_data():\n",
    "    # Read input\n",
    "    dataset = input(\"Please enter the path of the CSV file: \")\n",
    "    output_name = input(\"Please enter the name (and path) of the output file: \")\n",
    "    df = pd.read_csv(dataset)\n",
    "    \n",
    "    #preprocessing\n",
    "    trace_list = [] # list of traces\n",
    "\n",
    "    for name, group in df.groupby([\"case concept:name\"]):\n",
    "        trace_list.append(group['case concept:name'].tolist())\n",
    "\n",
    "    #define the function#\n",
    "    def find_list_features(list):\n",
    "        list_len = [len(i) for i in list]\n",
    "        return list_len\n",
    "\n",
    "    features = find_list_features(trace_list)\n",
    "\n",
    "    # Setting the limit for extreme traces\n",
    "    limit = np.percentile(features, 95)\n",
    "\n",
    "\n",
    "    # Calculating frequency of each trace\n",
    "    freq = collections.defaultdict(int)  # 0 by default\n",
    "    for x in itertools.chain.from_iterable(trace_list):\n",
    "        freq[x] += 1\n",
    "\n",
    "    # Filtering the frequency dictionary\n",
    "    filtered_dict = {k:v for k,v in freq.items() if v < limit}\n",
    "\n",
    "    # Making a list out of the keys\n",
    "    allowed_traces = [*filtered_dict]\n",
    "\n",
    "    df = df[df['case concept:name'].isin(allowed_traces)]\n",
    "\n",
    "    # Parse the timestamp and convert it into y-m-d form\n",
    "    df['event time:timestamp'] = pd.to_datetime(df['event time:timestamp'], format = '%d-%m-%Y %H:%M:%S.%f')\n",
    "\n",
    "    # Sort data by timestamp in ascending order\n",
    "    df.sort_values(['event time:timestamp'], axis=0, inplace=True)\n",
    "    \n",
    "    # split into train set and test set (80/20)\n",
    "    df_train, df_test = train_test_split(df, test_size=0.2, shuffle = False)\n",
    "    \n",
    "    # remove cases started in the training set\n",
    "    df_test = df_test[df_test['case concept:name'].isin(df_train['case concept:name'].values)]\n",
    "\n",
    "    # Reset index\n",
    "    df_train.reset_index(drop=True, inplace=True)\n",
    "    df_test.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return (df_train, df_test, output_name)\n",
    "\n",
    "# Functions for event and time prediction, either in one def like last time or split \n",
    "\n",
    "def prediction(df_train, df_test):\n",
    "    # Event prediction\n",
    "    df_sort = df_train.set_index(df_train.groupby('case concept:name').cumcount(), append = True)\n",
    "\n",
    "    df_sort['day of the week'] = df_sort['event time:timestamp'].dt.dayofweek\n",
    "\n",
    "    avg_day = df_sort.groupby(level=1)['day of the week'].apply(\n",
    "        lambda x: x.mean()\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Time prediction\n",
    "    # Assign position number to each event\n",
    "    df_train = df_train.set_index(df_train.groupby('case concept:name').cumcount(), append = True)\n",
    "    df_test = df_test.set_index(df_test.groupby('case concept:name').cumcount(), append = True)\n",
    "    \n",
    "    #calculate time since started for each case\n",
    "    df_train = df_train.assign(time_since_started=df_train.groupby('case concept:name')['event time:timestamp'].apply(lambda x: x - x.iloc[0]))\n",
    "    \n",
    "    #groupby case concept and calculate average for each position\n",
    "    avg_timespan = df_train.groupby(level=1)['time_since_started'].apply(\n",
    "        lambda x: x.astype('timedelta64[s]').mean()\n",
    "    )\n",
    "\n",
    "    # apply on test set\n",
    "    result = df_test.merge(avg_timespan, left_on = df_test.index.get_level_values(1).values, right_index = True, how = 'left')\n",
    "    \n",
    "    result['time_since_started']=result['time_since_started'].astype('timedelta64[s]')\n",
    "    result['time_prediction']= result.groupby('case concept:name')['event time:timestamp'].transform(lambda x: x.min())+ result['time_since_started']\n",
    "    \n",
    "    result = df_test.merge(avg_day, left_on = df_sort.index.get_level_values(1).values, right_index = True, how = 'left')\n",
    "    \n",
    "    result['day of the week'] = df_sort['event time:timestamp'].dt.dayofweek\n",
    "\n",
    "    # Adding a new column for the predicted week\n",
    "    result['predicted_week'] = result.groupby(level=1)['day of the week_x'].transform(lambda x: x) + result['day of the week_y']\n",
    "    # Converting float to int\n",
    "    result.predicted_week = result.predicted_week.astype(int)\n",
    "    # Making sure that no values are above 6 since we consider 0,...,6 as days of the week\n",
    "    result['predicted_week'] = np.where(result['predicted_week'] > 6, result['predicted_week'] - 6, result['predicted_week'])\n",
    "\n",
    "    # Finding the most common event per day of the week\n",
    "    l = len(result['event concept:name'])\n",
    "    lst = []\n",
    "    commonEventPerDay = []\n",
    "    dayList = result['day of the week_x'].tolist()\n",
    "    eventList = result['event concept:name'].tolist()\n",
    "\n",
    "    for j in range (0, 7):\n",
    "        for i in range (0, l):\n",
    "            if dayList[i] == j:\n",
    "                lst.append(eventList[i])\n",
    "        findMode = mode(lst)\n",
    "        commonEventPerDay.append(findMode)\n",
    "        list = []\n",
    "        \n",
    "    # Predicted Event = Most Common Event for that Day of the Week\n",
    "    result['predicted_event'] = result['predicted_week']\n",
    "    for i in range (0, 7):\n",
    "        result.loc[result['predicted_week'] == i, 'predicted_event'] = commonEventPerDay[i]\n",
    "    \n",
    "    result = result.drop('Unnamed: 0', axis=1)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def save_results(output_name):\n",
    "    result.to_csv(output_name + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter the path of the CSV file: C:\\Users\\HP\\Desktop\\2IOI0 DBL Process Mining\\2IOI0-DBL-Process-Mining-main\\datasets\\BPI Challenge 2017-training.csv\n",
      "Please enter the name (and path) of the output file: resultTool2\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Column not found: day of the week_x'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-0b83f4d2a56e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0msave_results\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-134a82d95ac4>\u001b[0m in \u001b[0;36mprediction\u001b[1;34m(df_train, df_test)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;31m# Adding a new column for the predicted week\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m     \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'predicted_week'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'day of the week_x'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'day of the week_y'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m     \u001b[1;31m# Converting float to int\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredicted_week\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredicted_week\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    273\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 275\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Column not found: {key}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    276\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gotitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Column not found: day of the week_x'"
     ]
    }
   ],
   "source": [
    "df_train, df_test, output_name = process_data()\n",
    "result = prediction(df_train, df_test)\n",
    "save_results(output_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.read_csv('try1.csv')\n",
    "final[final['case concept:name']==205433]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
